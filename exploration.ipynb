{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Restructuring\n",
    "\n",
    "- Grouping by user to get info on their commits and which target_interest their commits belong to in a quantitative way\n",
    "- Using the user groups, we can again group the df by user groups and time and now have very few groups, and we can do regression on their activity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdf = pd.DataFrame({\n",
    "    \"user\": df[\"user\"],\n",
    "#    \"time_week\" : list(df[\"time_week\"]),\n",
    "    \"target_interest_id\" : list(y),\n",
    "    \"cluster_id\" : list(cluster_semi_s_hdb.labels_)\n",
    "    })\n",
    "\n",
    "\n",
    "userdf[\"target_interest\"] = userdf[\"target_interest_id\"].apply(lambda x: target_interest[x-1])\n",
    "\n",
    "\n",
    "userdf = userdf.groupby(\"user\").agg(list).reset_index()\n",
    "\n",
    "\n",
    "print(len(userdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#userdf_dict = userdf[[\"user\",\"time_week\",\"target_interest_word\"]].copy()\n",
    "userdf_with_dict = userdf[[\"user\",\"target_interest\"]].copy()\n",
    "\n",
    "\n",
    "userdf_with_dict[\"target_interest_dict\"] = userdf[\"target_interest\"].apply(lambda x: dict_per_user(x, target_interest))\n",
    "\n",
    "userdf_with_dict.drop(columns=[\"target_interest\"], inplace=True)\n",
    "\n",
    "print(len(userdf_with_dict))\n",
    "\n",
    "userdf_with_dict.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising the userdf\n",
    "#userdf = (userdf-userdf.min())/(userdf.max()-userdf.min())\n",
    "\n",
    "target_interest_matrix = np.array(userdf[\"target_interest\"].apply(lambda x: dict_per_user(x, target_interest)))\n",
    "\n",
    "df_user_interest_matrix = pd.DataFrame(list(target_interest_matrix))\n",
    "\n",
    "target_interest_matrix = df_user_interest_matrix.to_numpy()\n",
    "\n",
    "df_user_interest_matrix.insert(0, \"user\", userdf[\"user\"])\n",
    "\n",
    "df_user_interest_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_interest_matrix[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_clusters = HDBSCAN(min_cluster_size=90, min_samples=10, metric='euclidean', cluster_selection_method='eom').fit(target_interest_matrix)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Full dimensionality clustering output:\n",
    "    Len of colab clusters: {len(colab_clusters.labels_)}\n",
    "    Number of clusters: {len(set(colab_clusters.labels_)) - 1}\n",
    "    Number of rows as outliers: {colab_clusters.labels_.tolist().count(-1)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_umap = UMAP(n_neighbors=15, min_dist=0.0).fit_transform(target_interest_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_resdf = pd.DataFrame({\n",
    "    \"x\" : colab_umap[:, 0], \n",
    "    \"y\" : colab_umap[:, 1], \n",
    "    \"cluster\" : colab_clusters.labels_\n",
    "})\n",
    "\n",
    "# with few clusters you can turn on and off outliers with the -1 label\n",
    "#colab_resdf = colab_resdf[colab_resdf[\"cluster\"] != -1]\n",
    "\n",
    "#turning cluster to str for discrete color\n",
    "colab_resdf[\"cluster\"] = colab_resdf[\"cluster\"].astype(str)\n",
    "\n",
    "fig_colab = px.scatter(colab_resdf, x=\"x\", y=\"y\", color=\"cluster\", title=\"Colab clustering\", width=800, height=800, range_x=[-25, 25], range_y=[-25, 25])\n",
    "\n",
    "fig_colab.show()\n",
    "\n",
    "# TODO add one visualisation without time grouping\n",
    "# this would give us the \"true\" user groups, and then we could see if they moved around without breaking up the group too much\n",
    "# also it is not a bug that there is overlap of clusters, as the clustering takes place before umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# - now we have to do this per user.\n",
    "# - we need to look at what a given user is \"comitting\" about interest wise, and then see which cluster that user is in\n",
    "# - then when we    title=\"Timeline of commits by interest\",                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user cluster time grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dict to connect username and cluster id\n",
    "userdId_groupID_dict = dict(zip(df_user_interest_matrix[\"user\"], colab_clusters.labels_))\n",
    "\n",
    "if len(userdId_groupID_dict) - len(df_user_interest_matrix) != 0:\n",
    "    print(\"WARNING: dict and userdf_ex are not the same length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupdf = pd.DataFrame({\n",
    "    \"user\": df[\"user\"],\n",
    "    \"time_sec\" : list(df[\"time_sec\"]),\n",
    "    \"target_interest_id\" : list(y),\n",
    "    })\n",
    "\n",
    "# mapping in the target interest\n",
    "usergroupdf[\"target_interest\"] = usergroupdf[\"target_interest_id\"].apply(lambda x: target_interest[x-1])\n",
    "\n",
    "# Setting cluster id on the users to get the cluster id for each user\n",
    "usergroupdf[\"user_group_id\"] = usergroupdf[\"user\"].apply(lambda x: userdId_groupID_dict[x])\n",
    "\n",
    "# Making time sec into time day\n",
    "usergroupdf[\"time_day\"] = usergroupdf[\"time_sec\"].apply(lambda x: x//(60*60*24))\n",
    "\n",
    "usergroupdf.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data interpolation\n",
    "\n",
    "- Insert nan rows per missing date\n",
    "- Fill nan with smoothed values\n",
    "  - (this is called interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day =  df[\"time_sec\"].tail(1).values[0] // (60*60*24)\n",
    "last_day = df[\"time_sec\"].head(1).values[0] // (60*60*24) \n",
    "\n",
    "print(f'''\n",
    "first day: {first_day}\n",
    "last day: {last_day}\n",
    "ammount of days in df: {(df[\"time_sec\"].head(1).values[0] - df[\"time_sec\"].tail(1).values[0]) // (60*60*24)}\n",
    "''')\n",
    "\n",
    "days_series = pd.Series(range(first_day, last_day + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupdf = usergroupdf.groupby([\"user_group_id\", \"time_day\"]).agg(list).reset_index()\n",
    "\n",
    "\n",
    "print(f'User group equal to cluster groups: {len(usergroupdf[\"user_group_id\"].unique()) == len(set(colab_clusters.labels_))}')\n",
    "print(len(usergroupdf))\n",
    "\n",
    "\n",
    "usergroupdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the interest matrix again for user groups\n",
    "usergroup_target_interest_matrix = np.array(usergroupdf[\"target_interest\"].apply(lambda x: dict_per_user(x, target_interest)))\n",
    "\n",
    "df_group_interest_matrix = pd.DataFrame(list(usergroup_target_interest_matrix))\n",
    "\n",
    "usergroup_target_interest_matrix = df_group_interest_matrix.to_numpy()\n",
    "\n",
    "#df_usergroup_interest_matrix = usergroupdf[[\"user_group_id\", \"time_day\", \"target_interest\"]].copy()\n",
    "\n",
    "df_group_interest_matrix.insert(0, \"user_group_id\", usergroupdf[\"user_group_id\"])\n",
    "df_group_interest_matrix.insert(1, \"time_day\", usergroupdf[\"time_day\"])\n",
    "\n",
    "\n",
    "df_group_interest_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_interest_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_frame = pd.DataFrame({\"time_day\" : days_series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making list of dfs based on usergroup\n",
    "\n",
    "usergroup_df_list = []\n",
    "\n",
    "for usergroup in df_group_interest_matrix[\"user_group_id\"].unique():\n",
    "  # interpolationg the missing days\n",
    "  \n",
    "  df_this_group = df_group_interest_matrix[df_group_interest_matrix[\"user_group_id\"] == usergroup]\n",
    "\n",
    "  df_this_group.drop(columns=[\"user_group_id\"], inplace=True)\n",
    "\n",
    "  df_this_group = pd.merge(days_frame, df_this_group, how=\"outer\", on=\"time_day\")\n",
    "\n",
    "  #df_this_group.set_index(\"time_day\", inplace=True)\n",
    "\n",
    "  for columns in df_this_group.columns:\n",
    "    if columns != \"time_day\":\n",
    "      df_this_group[columns][0] = 0.0\n",
    "      df_this_group[columns] = df_this_group[columns].astype(float)\n",
    "      df_this_group[columns].interpolate(method=\"spline\", order=3, inplace=True) # cubic, tried linear, but it was not as good\n",
    "      df_this_group.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "  #df_this_group = df_this_group.interpolate(method=\"pad\", axis=0)\n",
    "  \n",
    "  usergroup_df_list.append(df_this_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in usergroup_df_list:\n",
    "  print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroup_df_list[5].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroup_df_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Custom Model\n",
    "This will be scuffed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not need flatten as we do not have 2D input\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "\n",
    "        # nn.Sequential sets the layers in order\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "\n",
    "          # our input is 20\n",
    "            nn.Linear(20, 40),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # adding two more hidden layers\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "\n",
    "          # our output is 20\n",
    "            nn.Linear(40, 20),\n",
    "            nn.ReLU(),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "# taking one of the usergrop dfs to test\n",
    "sampledf = usergroup_df_list[1]\n",
    "\n",
    "print(len(sampledf))\n",
    "\n",
    "group_tensor = torch.tensor(sampledf.drop(columns=[\"time_day\"]).values, device=device).float()\n",
    "\n",
    "\n",
    "start = floor(len(group_tensor) * (7/10))\n",
    "end = len(group_tensor) - start\n",
    "\n",
    "group_tensor_train, group_tensor_test = torch.split(group_tensor, [start, end])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "{group_tensor.size()}\n",
    "{group_tensor_train.size()}\n",
    "{group_tensor_test.size()}\n",
    "\"\"\")\n",
    "\n",
    "len(group_tensor[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.NLLLoss()\n",
    "\n",
    "# this is the loss function we want to use? MSE is often used when predicting numerical stuff\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider cranking this up as the loss kept going down (maybe 20 would be sane)\n",
    "for epoch in range(80):\n",
    "  running_loss = 0\n",
    "\n",
    "  #prediction_row_tensor = torch.tensor([0,20]).to(device)\n",
    "\n",
    "  #prediction_row_tensor.cat(group_tensor_train[0])\n",
    "\n",
    "\n",
    "  #prediction_row_tensor = torch.cat((prediction_row_tensor, group_tensor_train[0]), dim=0)\n",
    "\n",
    "  prediction_row_tensor = group_tensor_train[0:1]\n",
    "\n",
    "\n",
    "  for i in range(len(group_tensor_train) - 1): # - 1 because we are getting the next row yeee\n",
    "    \n",
    "    this_row = group_tensor_train[i]\n",
    "    next_row = group_tensor_train[i+1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    next_row_prediction = model(this_row)\n",
    "\n",
    "    prediction_row_tensor = torch.cat((prediction_row_tensor, next_row_prediction[None, ...]), dim=0)\n",
    "\n",
    "    loss = criterion(next_row_prediction, next_row)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "  else:\n",
    "    print(f\"Epoch {epoch} - Training loss: {running_loss/len(group_tensor_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_row_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_tensor[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting / Looking at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictionns for the test set\n",
    "\n",
    "for row in group_tensor_test:\n",
    "    prediction_row_tensor = torch.cat((prediction_row_tensor, model(row)[None, ...]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prl = prediction_row_tensor.cpu().detach().numpy()\n",
    "gp = group_tensor.cpu().detach().numpy()\n",
    "\n",
    "fig = px.line(pd.DataFrame(list(prl))[start: ], height=600, width=1200)# [start: ]\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(pd.DataFrame(list(gp))[start: ], height=600, width=1200) # [start: ]\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring water simulation based prediction potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\"\"\n",
    "    Bounds of the uembs\n",
    "    \n",
    "    x axis:\n",
    "    {min(uembs[:,0])}\n",
    "    {max(uembs[:,0])}\n",
    "    \n",
    "    \"y axis\"\n",
    "    {min(uembs[:,1])}\n",
    "    {max(uembs[:,1])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan sette ramme til vann prediction på +- 25 på begge akser\n",
    "\n",
    "512 x 512*2 pixels i det spacet\n",
    "\n",
    "lage neste frame i animasjonen\n",
    "\n",
    "gi to frames av fortid\n",
    "- kan gi en frame per uke per bruker\n",
    "- kan ha en farge per bruker gruppe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rapids-39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3e30549315b0b530d1bf094e65f8bed842ab2ed21320e2e11ac70986a34aeb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
