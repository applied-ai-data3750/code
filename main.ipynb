{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbreviations:\n",
    "\n",
    "- `df` -> dataframe\n",
    "\n",
    "- `Series` -> column / kolonne\n",
    "\n",
    "- `embs` -> embeddings (numerical representation of words in a vector space, from our NLP AI model)\n",
    "\n",
    "- `uembs` -> umap embeddings (dimensionality reduction of embs, to make it easier to visualize, 768 -> 2)\n",
    "\n",
    "- `target_intersts` -> the 20 words we use as targets for our grouping of commit messages. We can se these, and we use them as targets for semi-supervision\n",
    "\n",
    "- `group_topics` -> the topics we generate based on message cluster, less important for the ML pipeline, more for the human reader to understand what the clusters are about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tensorflow-big.csv\", on_bad_lines='skip')\n",
    "\n",
    "len_start = len(df)\n",
    "\n",
    "df = df[[\"author_name\", \"time_sec\", \"subject\"]]\n",
    "\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"author_name\": \"user\", \n",
    "    \"time_sec\": \"time_sec\", \n",
    "    \"subject\": \"text\"\n",
    "})\n",
    "\n",
    "# dropping the two bots\n",
    "df = df[df[\"user\"] != \"A. Unique TensorFlower\"]\n",
    "df = df[df[\"user\"] != \"TensorFlower Gardener\"]\n",
    "\n",
    "len_drop_bots = len(df)\n",
    "\n",
    "\n",
    "# if your computer does not have GPU support, you can use a sample of the dataset instead to make it run in a reasonable time\n",
    "if device == \"cpu\": df = df.sample(frac=0.05)\n",
    "\n",
    "print(f\"\"\"\n",
    "Rows before droppings bots:   {len_start}\n",
    "Rows after dropping bots:     {len_drop_bots}\n",
    "Rows diff:                    {len_start - len_drop_bots}\n",
    "\"\"\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the commits are in order, and ready to be sliced timewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"text\" : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider Dropping A. Unique and Gardener\n",
    "\n",
    "df[\"user\"].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are intending to use the `text` field as a temporary substitue of `categoryRaw` which we wait to get from the schibsted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(string_cleaner)\n",
    "\n",
    "df[[\"text\", \"text_clean\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a new column for the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_week\"] = df[\"time_sec\"].apply(lambda x: x//604800)\n",
    "\n",
    "df.to_pickle(names[\"df\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping on users\n",
    "\n",
    "lag dictionary av å groupe på alle commit messages de har \n",
    "set groups på userId senere, så kan vi lage animation frames av hvordan grupper beveger seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfu - ABBR: Data Frame User grouped\n",
    "dfu = df[[\"text_clean\", \"time_sec\", \"time_week\", \"user\"]].copy()\n",
    "\n",
    "dfu = dfu.groupby([\"user\", \"time_week\"]).agg(list).reset_index()\n",
    "\n",
    "dfu[\"text_clean_join\"] = dfu[\"text_clean\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "dfu.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_per_user = dfu[\"user\"].value_counts().reset_index()\n",
    "\n",
    "print(len(weeks_per_user))\n",
    "\n",
    "weeks_per_user[weeks_per_user[\"user\"] > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Embeddings\n",
    "\n",
    "Getting the 768 dimensional embeddings for each commit message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if conf[\"fresh_data\"]: raise Exception\n",
    "    embs = pickle.load(open(names[f\"embs-{device}\"], 'rb'))\n",
    "    \n",
    "except:\n",
    "    embs = sbert_emb_getter(df[\"text_clean\"].to_numpy(), filename=names[f\"model-{device}\"])\n",
    "    pickle.dump(embs, open(names[f\"embs-{device}\"], 'wb'))\n",
    "\n",
    "    conf[\"fresh_embs\"] = True\n",
    "\n",
    "print(f\"fresh embs: {conf['fresh_embs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "We use UMAP to reduce the dimensionality of the embeddings from 768 to 2, so that we can visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_metric = \"euclidean\"\n",
    "\n",
    "try:\n",
    "    if conf[\"fresh_data\"]: raise Exception\n",
    "    uembs = pickle.load(open(names[f\"uembs-{device}\"], 'rb'))\n",
    "    \n",
    "except:\n",
    "    uembs = UMAP(n_neighbors=15, min_dist=0.0).fit_transform(embs)\n",
    "    pickle.dump(uembs, open(names[f\"uembs-{device}\"], 'wb'))\n",
    "\n",
    "    conf[\"fresh_uembs\"] = True\n",
    "\n",
    "print(f\"fresh uembs: {conf['fresh_uembs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make this plot just a trace, to fit in gridplot\n",
    "\n",
    "fig = px.scatter(\n",
    "  x=uembs[:,0], \n",
    "  y=uembs[:,1],\n",
    "  range_x = [-25, 25],\n",
    "  range_y = [-25, 25]\n",
    ")\n",
    "\n",
    "fig.update_layout(width=800, height=800)\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "# plotting to show how the embeddings are when just dimensionality reduction is used\n",
    "#fig_show_save(fig, \"umap-scatter\", show=conf[\"show_figs\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_2d = HDBSCAN(min_cluster_size=100, min_samples=20, metric='euclidean', cluster_selection_method='eom').fit(uembs)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "    2D\n",
    "    Number of clusters: {len(set(clusters_2d.labels_)) - 1}\n",
    "    Number of rows as outliers: {clusters_2d.labels_.tolist().count(-1)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking most common words\n",
    "\n",
    "First checking without filtering for stopwords, then checking with filtering for stopwords\n",
    "\n",
    "Then checking with filtering for english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = (\n",
    "    df[\"text_clean\"].apply(lambda x: (x.split(\" \")))\n",
    "    .explode()\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "vc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords has been imported from nltk\n",
    "s_words = stopwords.words('english')\n",
    "\n",
    "print(f\"\"\"\n",
    "    {type(s_words)}\n",
    "    {len(s_words)}\n",
    "    {s_words[0:10]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['add', 'fix', 'change', 'merge', 'update', 'test', 'remove', 'support', 'use', 'xla', 'build', 'request', 'pull', 'op', 'tests', 'tf', 'make', 'ops', 'error', 'api']\n"
     ]
    }
   ],
   "source": [
    "# vc where the index is not in the stopwords list\n",
    "vc = vc[~vc[\"index\"].isin(s_words)]\n",
    "\n",
    "# getting the top 20 words\n",
    "target_interest = list(vc.head(conf[\"interest_words\"] + 1)[\"index\"])\n",
    "\n",
    "# removing the space that becomes the first element in the list\n",
    "try:\n",
    "  target_interest.remove(\"\")\n",
    "except:\n",
    "  pass\n",
    "\n",
    "print(target_interest)\n",
    "\n",
    "names[\"target-interests\"] = \"data/target-interests.pkl\"\n",
    "\n",
    "pickle.dump(target_interest, open(names[\"target-interests\"], 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for seeing if a commit message belongs to a interest or if not\n",
    "threshold = 0.2\n",
    "\n",
    "if not conf[\"generate_interests\"]:\n",
    "    target_interest = interest_fixer(\"\"\"\n",
    "    fix add merge remove update pull request python docs tensorflow generated\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"using generated target_interest: {conf['generate_interests']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stopwords as a union with the nltk stopwords and the target_interest found by value counting all the words in the commit messages\n",
    "bonus_words = stopwords.words('english') + (target_interest)\n",
    "\n",
    "# dumping for the plotting notebook\n",
    "pickle.dump(bonus_words, open(names[\"bonus-words\"], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO make this use the get embeddings function\n",
    "# getting results.\n",
    "y, similarity = make_dataset(embs, targets=target_interest, model=model, target_threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)\n",
    "\n",
    "list(y).index(0)\n",
    "\n",
    "y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(target_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# liste av indexes\n",
    "target_interest_list = [target_interest[i-1] for i in y]\n",
    "\n",
    "print(len(target_interest_list) - len(df))\n",
    "\n",
    "print(target_interest_list[0:10])\n",
    "\n",
    "names[\"target-interest-list\"] = \"data/target-interest-list.pkl\"\n",
    "\n",
    "pickle.dump(target_interest_list, open(names[\"target-interest-list\"], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(target_interest_list)\n",
    "len(set(target_interest_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  if conf[\"fresh_data\"]: raise Exception\n",
    "\n",
    "  uemb_semi_s = pickle.load( open( names[f\"uembs-s-{device}\"], \"rb\" ) )\n",
    "  \n",
    "except:\n",
    "  # used to have just nn = 100 at 0.2 similiarity, and not the metric and target weight\n",
    "  # target weight is between 0 - 1, 0.5 is default, we used 1 for a while\n",
    "  uemb_semi_s = UMAP(n_neighbors=15, min_dist=0.0, target_weight=0.5).fit_transform(embs, y-1)\n",
    "  pickle.dump( uemb_semi_s, open( names[f\"uembs-s-{device}\"], \"wb\" ) )\n",
    "\n",
    "  conf[\"fresh_s_uembs\"] = True\n",
    "\n",
    "print(f\"fresh semi supervised uembs: {conf['fresh_s_uembs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_semi_s_hdb = HDBSCAN(min_cluster_size=100, min_samples=20, metric='euclidean', cluster_selection_method='eom').fit(uemb_semi_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster & Topic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2d = result_df_maker(uembs, clusters_2d.labels_, df[\"text_clean\"].to_numpy(), bonus_words=target_interest)\n",
    "\n",
    "vcr = result_2d[[\"cluster_label\", \"group_topics\"]].groupby([\"cluster_label\", \"group_topics\"])[\"group_topics\"].count().reset_index(name=\"commit_count\").sort_values(by=\"commit_count\", ascending=False).head(20)\n",
    "\n",
    "vcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2d_semi = result_df_maker(uemb_semi_s, y, df[\"text_clean\"].to_numpy(), bonus_words=target_interest)\n",
    "\n",
    "vcr = result_2d_semi[[\"cluster_label\", \"group_topics\"]].groupby([\"cluster_label\", \"group_topics\"])[\"group_topics\"].count().reset_index(name=\"commit_count\").sort_values(by=\"commit_count\", ascending=False).head(20)\n",
    "\n",
    "vcr\n",
    "\n",
    "# cluster labelis here interest label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding most common words in 20 most common group_topics to see if we need more stopwords\n",
    "vcr[\"group_topics\"].apply(lambda x: x.split(\" \")).explode().value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Result DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan gjøre regersjons øking per interesse, og velge hvilken som er mest likely ved et tidspunkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres = df[[\"text_clean\", \"time_sec\", \"time_week\", \"user\"]].copy()\n",
    "\n",
    "#dfres[\"time_week\"] = dfres[\"time_sec\"].apply(lambda x: datetime.fromtimestamp(x).isocalendar()[1])\n",
    "\n",
    "#dfres[\"time_week\"] = dfres[\"time_sec\"].apply(lambda x: x//604800)\n",
    "\n",
    "dfres[\"x\"] = uemb_semi_s[:, 0]\n",
    "\n",
    "dfres[\"y\"] = uemb_semi_s[:, 1]\n",
    "\n",
    "dfres[\"cluster\"] = cluster_semi_s_hdb.labels_\n",
    "\n",
    "\n",
    "\n",
    "dfres[\"interest_id\"] = list(y)\n",
    "\n",
    "# -1 to make up for adding 1 earlier\n",
    "dfres[\"target_interest\"] = dfres[\"interest_id\"].apply(lambda x: target_interest[x-1])\n",
    "\n",
    "# find topic by interest instead\n",
    "topic_dict = topic_by_clusterId(dfres[\"text_clean\"].to_numpy(), dfres[\"interest_id\"].to_numpy(), bonus_words=bonus_words)\n",
    "\n",
    "dfres[\"topic\"] = dfres[\"interest_id\"].apply(lambda x: \" \".join(list(topic_dict[x])))\n",
    "\n",
    "dfres = dfres[dfres[\"cluster\"] != -1]\n",
    "\n",
    "# Pickling the dfres for plotting in other notebook\n",
    "dfres.to_pickle(names[\"dfres\"])\n",
    "\n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Restructuring\n",
    "\n",
    "- Grouping by user to get info on their commits and which target_interest their commits belong to in a quantitative way\n",
    "- Using the user groups, we can again group the df by user groups and time and now have very few groups, and we can do regression on their activity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3803\n"
     ]
    }
   ],
   "source": [
    "userdf = pd.DataFrame({\n",
    "    \"user\": df[\"user\"],\n",
    "#    \"time_week\" : list(df[\"time_week\"]),\n",
    "    \"target_interest_id\" : list(y),\n",
    "    \"cluster_id\" : list(cluster_semi_s_hdb.labels_)\n",
    "    })\n",
    "\n",
    "\n",
    "userdf[\"target_interest\"] = userdf[\"target_interest_id\"].apply(lambda x: target_interest[x-1])\n",
    "\n",
    "\n",
    "userdf = userdf.groupby(\"user\").agg(list).reset_index()\n",
    "\n",
    "\n",
    "print(len(userdf))\n",
    "\n",
    "userdf.to_pickle(names[\"df-user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usergroupdf = pd.DataFrame({\n",
    "    \"user\": df[\"user\"],\n",
    "    \"time_sec\" : list(df[\"time_sec\"]),\n",
    "    \"target_interest_id\" : list(y),\n",
    "    })\n",
    "\n",
    "# mapping in the target interest\n",
    "usergroupdf[\"target_interest\"] = usergroupdf[\"target_interest_id\"].apply(lambda x: target_interest[x-1])\n",
    "\n",
    "# Setting cluster id on the users to get the cluster id for each user\n",
    "usergroupdf[\"user_group_id\"] = usergroupdf[\"user\"].apply(lambda x: userdId_groupID_dict[x])\n",
    "\n",
    "# Making time sec into time day\n",
    "usergroupdf[\"time_day\"] = usergroupdf[\"time_sec\"].apply(lambda x: x//(60*60*24))\n",
    "\n",
    "usergroupdf.sample(5)\n",
    "\n",
    "names[\"user-group-df\"] = \"data/df-user-group.pkl\"\n",
    "\n",
    "usergroupdf.to_pickle(names[\"df-user-group\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rapids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa509c4d6160beb2d784a490a2868c8fb0afdf9928784f83fac5c0781a52bf4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
